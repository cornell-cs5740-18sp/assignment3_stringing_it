{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading datasets\n",
      "Training complete in --- 680.504508018 seconds ---\n",
      "91\n",
      "Dev complete in --- 0.373021125793 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import time\n",
    "\n",
    "\"\"\" Contains the part of speech tagger class. \"\"\"\n",
    "\n",
    "def load_data(sentence_file, tag_file=None, data_type=None):\n",
    "    \"\"\"Loads data from two files: one containing sentences and one containing tags.\n",
    "\n",
    "    tag_file is optional, so this function can be used to load the test data.\n",
    "\n",
    "    Suggested to split the data by the document-start symbol.\n",
    "    \n",
    "    INPUT : file names, data type\n",
    "    OUTPUT : tuple (token, tag)\n",
    "\n",
    "    \"\"\"\n",
    "    if data_type==\"train\" or data_type==\"dev\":\n",
    "        tokens = pd.read_csv(sentence_file,delimiter=\",\",header=0)[\"word\"].tolist()\n",
    "        tags = pd.read_csv(tag_file,delimiter=\",\",header=0)[\"tag\"].tolist()\n",
    "        \n",
    "        data = []\n",
    "        for i in range(len(tokens)):\n",
    "            data.append((tokens[i],tags[i]))\n",
    "    else:\n",
    "        tokens = pd.read_csv(sentence_file,delimiter=\",\",header=0)[\"word\"].tolist()\n",
    "        \n",
    "        data = []\n",
    "        for i in range(len(tokens)):\n",
    "            data.append((tokens[i],\"NN\")) # setting default tag as NOUN NN\n",
    "    \n",
    "    return data\n",
    "\n",
    "def evaluate_per_token(data, model):\n",
    "    \"\"\"Evaluates the POS model on some sentences and gold tags.\n",
    "\n",
    "    This model can compute a few different accuracies:\n",
    "        - whole-sentence accuracy\n",
    "        - per-token accuracy\n",
    "        - compare the probabilities computed by different styles of decoding\n",
    "\n",
    "    You might want to refactor this into several different evaluation functions.\n",
    "    \n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    act_tags = []\n",
    "    pred_tags = []\n",
    "    count_right = 0\n",
    "    for token,tag in data:\n",
    "        pred = model.baseline_token_probability(token)\n",
    "        pred_tags.append(pred)\n",
    "        act_tags.append(tag)\n",
    "        if pred==tag:\n",
    "            count_right+=1\n",
    "    acc = (count_right*100/len(act_tags))\n",
    "    print acc\n",
    "    print (\"Dev complete in --- %s seconds ---\" % (time.time() - start_time))\n",
    "    return acc\n",
    "\n",
    "def evaluate_whole_sentence(data, model):\n",
    "    \"\"\"Evaluates the POS model on some sentences and gold tags.\n",
    "\n",
    "    This model can compute a few different accuracies:\n",
    "        - whole-sentence accuracy\n",
    "        - per-token accuracy\n",
    "        - compare the probabilities computed by different styles of decoding\n",
    "\n",
    "    You might want to refactor this into several different evaluation functions.\n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class POSTagger():\n",
    "    \n",
    "    token_tag_prob = {} #for every token-tag pair calculates probability from training data\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the tagger model parameters and anything else necessary. \"\"\"         \n",
    "        pass\n",
    "\n",
    "    def train(self, data):\n",
    "        \"\"\"Trains the model by computing transition and emission probabilities.\n",
    "\n",
    "        You should also experiment:\n",
    "            - smoothing.\n",
    "            - N-gram models with varying N.\n",
    "        \n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        for token,tag in data:\n",
    "            if token not in self.token_tag_prob.keys():\n",
    "                tag_set = {}\n",
    "                tag_set[tag] = 1\n",
    "                self.token_tag_prob[token] = tag_set\n",
    "            else:\n",
    "                tag_set = self.token_tag_prob[token]\n",
    "                if tag not in tag_set.keys():\n",
    "                    tag_set[tag] = 1\n",
    "                else:\n",
    "                    tag_set[tag] += 1\n",
    "        print (\"Training complete in --- %s seconds ---\" % (time.time() - start_time))\n",
    "        return\n",
    "    \n",
    "    def baseline_token_probability(self, token):\n",
    "        try:\n",
    "            possible_tags = self.token_tag_prob[token]\n",
    "            pred = \"\"\n",
    "            count = 0\n",
    "            tot = 0.0\n",
    "        \n",
    "            for tag,val in possible_tags.items():\n",
    "                if val>=count:\n",
    "                    count = val\n",
    "                    pred = tag\n",
    "                tot+=val\n",
    "        except:\n",
    "            return \"NNP\" # Defaulting to Noun\n",
    "        return pred\n",
    "\n",
    "    def sequence_probability(self, sequence, tags):\n",
    "        \"\"\"Computes the probability of a tagged sequence given the emission/transition\n",
    "        probabilities.\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    def inference(self, sequence):\n",
    "        \"\"\"Tags a sequence with part of speech tags.\n",
    "\n",
    "        You should implement different kinds of inference (suggested as separate\n",
    "        methods):\n",
    "\n",
    "            - greedy decoding\n",
    "            - decoding with beam search\n",
    "            - viterbi\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pos_tagger = POSTagger()\n",
    "    \n",
    "    # USE APPROPRIATE FILE PATH\n",
    "    train_data = load_data(\"../data/train_x.csv\", \"../data/train_y.csv\",\"train\")\n",
    "    dev_data = load_data(\"../data/dev_x.csv\", \"../data/dev_y.csv\",\"dev\")\n",
    "    test_data = load_data(\"../data/test_x.csv\")\n",
    "\n",
    "    print \"Done loading datasets\"\n",
    "    pos_tagger.train(train_data)\n",
    "\n",
    "    # Experiment with your decoder using greedy decoding, beam search, viterbi...\n",
    "\n",
    "    # Here you can also implement experiments that compare different styles of decoding,\n",
    "    # smoothing, n-grams, etc.\n",
    "    evaluate_per_token(dev_data, pos_tagger)\n",
    "\n",
    "    # Predict tags for the test set\n",
    "    #test_predictions = []\n",
    "    #for sentence in test_data:\n",
    "    #    test_predictions.extend(pos_tagger.inference(sentence)\n",
    "    \n",
    "    # Write them to a file to update the leaderboard\n",
    "    # TODO\n",
    "    #test_predictions = pd.DataFrame({\"id\":np.arange(len(test_predictions)), \"tag\":test_predictions})\n",
    "    #test_predictions.to_csv(\"../results/test_y.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
