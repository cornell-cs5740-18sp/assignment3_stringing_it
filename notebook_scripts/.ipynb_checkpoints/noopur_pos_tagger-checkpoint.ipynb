{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading datasets\n",
      "Training complete in --- 702.780346155 seconds ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate 'str' and 'int' objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7b6b56b6fe2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# smoothing, n-grams, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mtrain_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_per_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0mdev_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_per_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_per_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7b6b56b6fe2e>\u001b[0m in \u001b[0;36mevaluate_per_token\u001b[0;34m(data, model, datatype)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount_right\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Train set accuracy : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate 'str' and 'int' objects"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import time\n",
    "\n",
    "\"\"\" Contains the part of speech tagger class. \"\"\"\n",
    "\n",
    "def load_data(sentence_file, tag_file=None, data_type=None):\n",
    "    \"\"\"Loads data from two files: one containing sentences and one containing tags.\n",
    "\n",
    "    tag_file is optional, so this function can be used to load the test data.\n",
    "\n",
    "    Suggested to split the data by the document-start symbol.\n",
    "    \n",
    "    INPUT : file names, data type\n",
    "    OUTPUT : tuple (token, tag)\n",
    "\n",
    "    \"\"\"\n",
    "    if data_type==\"train\" or data_type==\"dev\":\n",
    "        tokens = pd.read_csv(sentence_file,delimiter=\",\",header=0)[\"word\"].tolist()\n",
    "        tags = pd.read_csv(tag_file,delimiter=\",\",header=0)[\"tag\"].tolist()\n",
    "        \n",
    "        data = []\n",
    "        for i in range(len(tokens)):\n",
    "            data.append((tokens[i],tags[i]))\n",
    "    else:\n",
    "        tokens = pd.read_csv(sentence_file,delimiter=\",\",header=0)[\"word\"].tolist()\n",
    "        \n",
    "        data = []\n",
    "        for i in range(len(tokens)):\n",
    "            data.append((tokens[i],\"NN\")) # setting default tag as NOUN NN\n",
    "    \n",
    "    return data\n",
    "\n",
    "def evaluate_per_token(data, model, datatype=\"None\"):\n",
    "    \"\"\"Evaluates the POS model on some sentences and gold tags.\n",
    "\n",
    "    This model can compute a few different accuracies:\n",
    "        - whole-sentence accuracy\n",
    "        - per-token accuracy\n",
    "        - compare the probabilities computed by different styles of decoding\n",
    "\n",
    "    You might want to refactor this into several different evaluation functions.\n",
    "    \n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    act_tags = []\n",
    "    pred_tags = []\n",
    "    count_right = 0\n",
    "    for token,tag in data:\n",
    "        pred = model.baseline_token_probability(token)\n",
    "        pred_tags.append(pred)\n",
    "        act_tags.append(tag)\n",
    "        if pred==tag:\n",
    "            count_right+=1\n",
    "    \n",
    "    if datatype==\"train\":\n",
    "        acc = (count_right*100/len(act_tags))\n",
    "        print (\"Train set accuracy : \" + str(acc))\n",
    "    \n",
    "    if datatype==\"dev\":\n",
    "        acc = (count_right*100/len(act_tags))\n",
    "        print (\"Dev set accuracy : \" + str(acc))\n",
    "    \n",
    "    print (\"Evaluation complete in --- %s seconds ---\" % (time.time() - start_time))\n",
    "    return pred_tags\n",
    "\n",
    "def evaluate_whole_sentence(data, model):\n",
    "    \"\"\"Evaluates the POS model on some sentences and gold tags.\n",
    "\n",
    "    This model can compute a few different accuracies:\n",
    "        - whole-sentence accuracy\n",
    "        - per-token accuracy\n",
    "        - compare the probabilities computed by different styles of decoding\n",
    "\n",
    "    You might want to refactor this into several different evaluation functions.\n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class POSTagger():\n",
    "    \n",
    "    token_tag_prob = {} #for every token-tag pair calculates probability from training data\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the tagger model parameters and anything else necessary. \"\"\"         \n",
    "        pass\n",
    "\n",
    "    def train(self, data):\n",
    "        \"\"\"Trains the model by computing transition and emission probabilities.\n",
    "\n",
    "        You should also experiment:\n",
    "            - smoothing.\n",
    "            - N-gram models with varying N.\n",
    "        \n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        for token,tag in data:\n",
    "            if token not in self.token_tag_prob.keys():\n",
    "                tag_set = {}\n",
    "                tag_set[tag] = 1\n",
    "                self.token_tag_prob[token] = tag_set\n",
    "            else:\n",
    "                tag_set = self.token_tag_prob[token]\n",
    "                if tag not in tag_set.keys():\n",
    "                    tag_set[tag] = 1\n",
    "                else:\n",
    "                    tag_set[tag] += 1\n",
    "        print (\"Training complete in --- %s seconds ---\" % (time.time() - start_time))\n",
    "        return\n",
    "    \n",
    "    def baseline_token_probability(self, token):\n",
    "        try:\n",
    "            possible_tags = self.token_tag_prob[token]\n",
    "            pred = \"\"\n",
    "            count = 0\n",
    "            tot = 0.0\n",
    "        \n",
    "            for tag,val in possible_tags.items():\n",
    "                if val>=count:\n",
    "                    count = val\n",
    "                    pred = tag\n",
    "                tot+=val\n",
    "        except:\n",
    "            return \"NNP\" # Defaulting to Noun\n",
    "        return pred\n",
    "\n",
    "    def sequence_probability(self, sequence, tags):\n",
    "        \"\"\"Computes the probability of a tagged sequence given the emission/transition\n",
    "        probabilities.\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    def inference(self, sequence):\n",
    "        \"\"\"Tags a sequence with part of speech tags.\n",
    "\n",
    "        You should implement different kinds of inference (suggested as separate\n",
    "        methods):\n",
    "\n",
    "            - greedy decoding\n",
    "            - decoding with beam search\n",
    "            - viterbi\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pos_tagger = POSTagger()\n",
    "    \n",
    "    # USE APPROPRIATE FILE PATH\n",
    "    train_data = load_data(\"../data/train_x.csv\", \"../data/train_y.csv\",\"train\")\n",
    "    dev_data = load_data(\"../data/dev_x.csv\", \"../data/dev_y.csv\",\"dev\")\n",
    "    test_data = load_data(\"../data/test_x.csv\")\n",
    "\n",
    "    print \"Done loading datasets\"\n",
    "    pos_tagger.train(train_data)\n",
    "\n",
    "    # Experiment with your decoder using greedy decoding, beam search, viterbi...\n",
    "\n",
    "    # Here you can also implement experiments that compare different styles of decoding,\n",
    "    # smoothing, n-grams, etc.\n",
    "    \n",
    "    train_predictions = evaluate_per_token(train_data, pos_tagger, \"train\")\n",
    "    dev_predictions = evaluate_per_token(dev_data, pos_tagger, \"dev\")\n",
    "    test_predictions = evaluate_per_token(test_data,pos_tagger)\n",
    "    \n",
    "    # Predict tags for the test set MUST move to evaluate_whole_sentence()\n",
    "    #test_predictions = []\n",
    "    #for sentence in test_data:\n",
    "    #    test_predictions.extend(pos_tagger.inference(sentence))\n",
    "    \n",
    "    # Write them to a file to update the leaderboard\n",
    "    # TODO\n",
    "    test_predictions = pd.DataFrame({\"id\":np.arange(len(test_predictions)), \"tag\":test_predictions})\n",
    "    test_predictions.to_csv(\"../results/test_y.csv\", sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
