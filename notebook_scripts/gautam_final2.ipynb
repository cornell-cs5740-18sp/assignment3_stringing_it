{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentences(tokens,tags):\n",
    "    \"\"\"\n",
    "    Function converts list of words into sentences with sentences of corresponding tags\n",
    "\n",
    "    INPUT : Dataframe of tokens, Dataframe of tags\n",
    "\n",
    "    OUTPUT : Zip of list of sentences, list of tags sentences\n",
    "    \"\"\"\n",
    "    data = tokens.join(tags, on=\"id\", how = \"inner\", rsuffix = \"_tag\").drop(\"id_tag\",axis=1)\n",
    "    sentences = []\n",
    "    tags_list = []\n",
    "    temp_tokens = []\n",
    "    temp_tags = []\n",
    "    for row in data.itertuples():\n",
    "        word = row[2]\n",
    "        tag = row[3]\n",
    "        if word!='-DOCSTART-' and word!='.':\n",
    "            temp_tokens.append(word)\n",
    "            temp_tags.append(tag)\n",
    "        if word=='.':\n",
    "            sentences.append(' '.join(temp_tokens) + ' .')\n",
    "            tags_list.append(' '.join(temp_tags) + ' .')\n",
    "            temp_tokens = []\n",
    "            temp_tags = []\n",
    "    \n",
    "    return zip(sentences,tags_list)\n",
    "\n",
    "def load_data(sentence_file, tag_file=None):\n",
    "    \"\"\"Loads data from two files: one containing sentences and one containing tags.\n",
    "\n",
    "    tag_file is optional, so this function can be used to load the test data.\n",
    "\n",
    "    Suggested to split the data by the document-start symbol.\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = pd.read_csv(sentence_file)\n",
    "    if tag_file:\n",
    "        tags = pd.read_csv(tag_file)\n",
    "    else:\n",
    "        #dummy tags for test file\n",
    "        tags = pd.DataFrame()\n",
    "        tags['id'] = range(len(tokens))\n",
    "        tags['tag'] = ['NNP']*len(tokens)\n",
    "\n",
    "    return make_sentences(tokens,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"../data/train_x.csv\", \"../data/train_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating word_count dictionary\n",
    "word_count = Counter()\n",
    "for sentence in zip(*train_data)[0]:\n",
    "    for word in sentence.split(' '):\n",
    "        word_count[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_word(word, idx):\n",
    "    if word.isdigit():\n",
    "        if len(word) == 2:\n",
    "            return 'twoDigitNum'\n",
    "        elif len(word) == 4:\n",
    "            return 'fourDigitNum'\n",
    "        else:\n",
    "            return 'othernum'\n",
    "    elif word.isalpha():\n",
    "        if word.islower():\n",
    "            return 'lowercase'\n",
    "        elif word.isupper():\n",
    "            return 'allCaps'\n",
    "        elif word[0].isupper() and word[1:].islower() and idx == 0:\n",
    "            return 'firstWord'\n",
    "        elif word[0].isupper() and word[1:].islower():\n",
    "            return 'initCap'\n",
    "        else:\n",
    "            return 'other'    \n",
    "    else:\n",
    "        without_punct = word.translate(None, string.punctuation)\n",
    "        if without_punct.isdigit() and ',' in word:\n",
    "            return 'containsDigitAndComma'\n",
    "        elif without_punct.isdigit() and '-' in word:\n",
    "            return 'containsDigitAndDash'\n",
    "        elif without_punct.isdigit() and '/' in word:\n",
    "            return 'containsDigitAndSlash'\n",
    "        elif without_punct.isdigit() and '.' in word:\n",
    "            return 'containsDigitAndPeriod'\n",
    "        elif len(word) == 2 and word[0].isupper() and word[1] == '.':\n",
    "            return 'capPeriod'\n",
    "        elif without_punct.isalnum():\n",
    "            return 'containsDigitAndAlpha'\n",
    "        else:\n",
    "            return 'other'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nGramTagger(n):\n",
    "    dic = {}\n",
    "    tags = zip(*train_data)[1]\n",
    "    for line in tags:\n",
    "        line = line.split(' ')\n",
    "        line = ['*']*n + line\n",
    "        for i in range(n,len(line)):\n",
    "            if n==1:\n",
    "                item = line[i]\n",
    "            else:\n",
    "                item = tuple(line[i-n:i])\n",
    "            if item in dic:\n",
    "                dic[item]+=1\n",
    "            else:\n",
    "                dic[item]=1\n",
    "    return dic\n",
    "\n",
    "def wordTagger():\n",
    "    dic = defaultdict(int) \n",
    "    for line1,line2 in train_data:\n",
    "        for word,tag in zip(line1.split(' '),line2.split(' ')):\n",
    "            dic[(word,tag)]+=1\n",
    "            if word_count[word]<5:\n",
    "                idx = line1.split(' ').index(word)\n",
    "                category = categorize_word(word, idx)\n",
    "                dic[(category,tag)]+=1\n",
    "    return dic\n",
    "\n",
    "unigram = nGramTagger(1)\n",
    "bigram = nGramTagger(2)\n",
    "trigram = nGramTagger(3)\n",
    "fourgram = nGramTagger(4)\n",
    "\n",
    "wordtag = wordTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def good_turing_smoothing(dic):\n",
    "#     reverse_dic = {}\n",
    "#     k=5\n",
    "#     #smoothing for rare words\n",
    "#     for key,value in dic.items():\n",
    "#         if value<=5:\n",
    "#             reverse_dic[value] = reverse_dic.get(value,0)+1\n",
    "    \n",
    "#     for key,value in dic.items():\n",
    "#         if value<5:\n",
    "#             dic[key] = (value+1)*reverse_dic[value+1]/reverse_dic[value]\n",
    "#     print reverse_dic\n",
    "#     #handling for unseen words or combinations\n",
    "# #     dic['UNK'] = reverse_dic.get(1,1)\n",
    "    \n",
    "#     return dic\n",
    "    \n",
    "# unigram = good_turing_smoothing(unigram)\n",
    "# bigram = good_turing_smoothing(bigram)\n",
    "# trigram = good_turing_smoothing(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNSmoothing():\n",
    "    probs = {}\n",
    "    total = len(trigram)\n",
    "    for tag in unigram.keys():\n",
    "        c=0\n",
    "        for item in trigram.keys():\n",
    "            if tag==item[2]:\n",
    "                c+=1\n",
    "        probs[tag]=float(c)/total\n",
    "        \n",
    "    bigram_kn_counts={}\n",
    "    for key in bigram.keys():\n",
    "        for item in trigram.keys():\n",
    "            if key[0]==item[0] and key[1]==item[1]:\n",
    "                bigram_kn_counts[key] = bigram_kn_counts.get(key,0)+1\n",
    "    \n",
    "    return probs, bigram_kn_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs,bigram_kn_counts = KNSmoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_set = unigram.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_bigram_counts = sum(bigram.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q(tag_penult,tag_prev,tag_current):\n",
    "    k=1\n",
    "    d=0.75\n",
    "\n",
    "    num = float(trigram.get((tag_penult, tag_prev, tag_current),0.0)) + k -d\n",
    "    den = float(bigram.get((tag_penult, tag_prev), 0.0)) + k*len(bigram)\n",
    "    lambd = float(d*bigram_kn_counts.get((tag_penult,tag_prev),0.0))/den\n",
    "    Pcont  = probs.get(tag_current,0.0)\n",
    "    \n",
    "    return (num/den) + (lambd*Pcont)\n",
    "    \n",
    "\n",
    "def get_e(word,tag,idx):\n",
    "    if word not in word_count:\n",
    "        category = categorize_word(word,idx)\n",
    "        return float(wordtag[(category,tag)])/unigram[tag]\n",
    "    else:\n",
    "        return float(wordtag[(word,tag)])/unigram[tag]\n",
    "  \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_distibution(k):\n",
    "    prob = []\n",
    "    tag_list = zip(*unigram.keys())[0]\n",
    "    for key1 in tag_list :\n",
    "        for key2 in tag_list:\n",
    "            if (key1,key2) in bigram:\n",
    "                num = bigram.get((key1,key2),0.0) + k\n",
    "            else:\n",
    "                num=k\n",
    "            den = unigram.get(key1,0.0) + (k*len(tag_list))\n",
    "            prob.append(float(num)/den if den!=0 else 0.0)\n",
    "    print prob.count(0.0)\n",
    "    plt.plot(sorted(prob,reverse=True))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_probability(self, sequence, tags):\n",
    "        \"\"\"Computes the probability of a tagged sequence given the emission/transition\n",
    "        probabilities.\n",
    "        \"\"\"\n",
    "        tag_penult = '*'\n",
    "        tag_prev = '*'\n",
    "        prod = 1\n",
    "        for word, tag in zip(sequence,tags):\n",
    "            q = self.get_q(tag,tag_prev,tag_penult)\n",
    "            e = self.get_e(word,tag)\n",
    "            tag_penult = tag_prev\n",
    "            tag_prev = tagger\n",
    "            prod *= q*e\n",
    "\n",
    "        return prod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sequence,mode):\n",
    "        \"\"\"Tags a sequence with part of speech tags.\n",
    "\n",
    "        You should implement different kinds of inference (suggested as separate\n",
    "        methods):\n",
    "\n",
    "            - greedy decoding\n",
    "            - decoding with beam search\n",
    "            - viterbi\n",
    "        \"\"\"\n",
    "        #Method 1: Greedy Decoding\n",
    "        if mode.lower()=='greedy':\n",
    "            tag_sequence = []\n",
    "            tag_penult = '*'\n",
    "            tag_prev = '*'\n",
    "            for idx,word in enumerate(sequence.split(' ')):\n",
    "                scores = []\n",
    "                for tag in tag_set:\n",
    "                    scores.append(get_q(tag_penult,tag_prev,tag)*get_e(word,tag,idx))\n",
    "                final_tag = tag_set[np.argmax(scores)]\n",
    "                tag_sequence.append(final_tag)\n",
    "                tag_penult = tag_prev\n",
    "                tag_prev = final_tag\n",
    "                \n",
    "            return tag_sequence\n",
    "        \n",
    "        #Method 2: Beam Search\n",
    "        elif mode.lower()=='beam':\n",
    "            k=1\n",
    "            sequence = sequence.split()\n",
    "            best_sequences = [['*','*']]\n",
    "            for idx,word in enumerate(sequence):\n",
    "                scores = {}\n",
    "                for item in best_sequences:\n",
    "                    item = list(item)\n",
    "                    tag_penult = item[-2]\n",
    "                    tag_prev = item[-1]\n",
    "                    for tag in tag_set:\n",
    "                        score = get_q(tag_penult,tag_prev,tag)*get_e(word,tag,idx)\n",
    "                        item.append(tag)\n",
    "                        scores[tuple(item)] = score\n",
    "                        item.pop()\n",
    "                topk_scores = sorted(scores.items(), key=lambda x: x[1], reverse = True)\n",
    "                best_sequences = list(zip(*topk_scores)[0][:k])\n",
    "            \n",
    "            best_sequence =  best_sequences[0][2:]\n",
    "            \n",
    "\n",
    "            return best_sequence\n",
    "        \n",
    "        elif mode.lower()=='viterbi':\n",
    "            tag_sequence= []\n",
    "            path = {}\n",
    "            path['*','*'] = []\n",
    "            pi_func = defaultdict(float)\n",
    "            pi_func[(0, \"*\", '*')] = 0.0\n",
    "\n",
    "            # v = Tag current, u = Tag previous, w = Tag penult\n",
    "            sequence = sequence.split(\" \")\n",
    "            n = len(sequence)\n",
    "\n",
    "            for k in range(1,n+1):\n",
    "                temp_path = {}\n",
    "                for u in possible_tags(k-1):\n",
    "                    for v in possible_tags(k):\n",
    "                        max_tag = \"\"\n",
    "                        max_score = float(\"-Inf\")\n",
    "                        for w in possible_tags(k - 2):\n",
    "                            score = pi_func.get((k-1, w, u),float('-Inf'))*get_q(w,u,v)*get_e(sequence[k-1],v,k-1)\n",
    "                            if score > max_score:\n",
    "                                max_score = score\n",
    "                                max_tag = w\n",
    "                        pi_func[(k, u, v)] = max_score\n",
    "                        temp_path[u,v] = path[max_tag,u] + [v]\n",
    "                path = temp_path\n",
    "\n",
    "            prob,umax,vmax = max([(pi_func.get((n,u,v))*get_q(u,v,'.'),u,v) for u in possible_tags(n-1) for v in possible_tags(n)])\n",
    "\n",
    "            return path[umax,vmax]\n",
    "        \n",
    "def possible_tags(k):\n",
    "    if k in (-1, 0):\n",
    "        return set('*')\n",
    "    else:\n",
    "        return tag_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "242559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9456627047440004, 0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(dev_data,'beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordtag[('firstWord','PRP$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('firstWord', 'CD') 16\n",
      "('firstWord', 'PRP') 0\n",
      "('firstWord', 'CC') 1\n",
      "('firstWord', 'UH') 3\n",
      "('firstWord', 'WDT') 2\n",
      "('firstWord', '``') 0\n",
      "('firstWord', 'VB') 41\n",
      "('firstWord', 'VBP') 0\n",
      "('firstWord', 'WP$') 0\n",
      "('firstWord', 'VBN') 83\n",
      "('firstWord', 'RBR') 2\n",
      "('firstWord', 'RB') 137\n",
      "('firstWord', 'WP') 0\n",
      "('firstWord', 'JJR') 15\n",
      "('firstWord', 'RBS') 1\n",
      "('firstWord', 'LS') 0\n",
      "('firstWord', 'JJS') 1\n",
      "('firstWord', 'EX') 0\n",
      "('firstWord', ')') 0\n",
      "('firstWord', ',') 0\n",
      "('firstWord', 'JJ') 156\n",
      "('firstWord', 'PRP$') 0\n",
      "('firstWord', 'POS') 0\n",
      "('firstWord', 'NNPS') 8\n",
      "('firstWord', 'VBG') 177\n",
      "('firstWord', 'PDT') 0\n",
      "('firstWord', 'VBD') 5\n",
      "('firstWord', '$') 0\n",
      "('firstWord', ':') 0\n",
      "('firstWord', 'TO') 0\n",
      "('firstWord', 'IN') 28\n",
      "('firstWord', 'MD') 5\n",
      "('firstWord', \"''\") 0\n",
      "('firstWord', 'NN') 261\n",
      "('firstWord', 'WRB') 0\n",
      "('firstWord', '(') 0\n",
      "('firstWord', '.') 0\n",
      "('firstWord', 'SYM') 0\n",
      "('firstWord', 'FW') 6\n",
      "('firstWord', 'DT') 0\n",
      "('firstWord', 'RP') 0\n",
      "('firstWord', 'NNS') 302\n",
      "('firstWord', 'NNP') 672\n",
      "('firstWord', 'VBZ') 3\n",
      "('firstWord', '#') 0\n"
     ]
    }
   ],
   "source": [
    "for key in wordtag.keys():\n",
    "    if key[0]=='firstWord':\n",
    "        print key, wordtag[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data,mode):\n",
    "    individual_score = 0\n",
    "    sentence_score = 0\n",
    "    total_word_count = 0\n",
    "    final_results = []\n",
    "    idx=0\n",
    "    for sentence, tag_sequence in data:\n",
    "        #checking number of sentences processed to gauge runtime\n",
    "        idx+=1\n",
    "        if idx%1000==0:\n",
    "            print idx\n",
    "        tag_sequence = tag_sequence.split(' ')\n",
    "        result = inference(sentence,mode)\n",
    "        final_results += result\n",
    "        if result==tag_sequence:\n",
    "            sentence_score+=1\n",
    "            individual_score+=len(result)\n",
    "            total_word_count += len(result)\n",
    "        else:\n",
    "            for predicted, actual in zip(result,tag_sequence):\n",
    "                total_word_count+=1\n",
    "                if predicted==actual:\n",
    "                    individual_score+=1\n",
    "    \n",
    "    save_results(final_results,'dev')\n",
    "    \n",
    "    return float(individual_score)/total_word_count, float(sentence_score)/len(data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(predicted_tags, mode):\n",
    "\n",
    "    if mode.lower()=='dev':\n",
    "        data_x = pd.read_csv('../data/dev_x.csv')\n",
    "        data_y = pd.read_csv('../data/dev_y.csv')\n",
    "    \n",
    "    elif mode.lower()=='test':\n",
    "        data_x = pd.read_csv('../data/test_x.csv')\n",
    "#         data_y = pd.read_csv('../results/test_y.csv')\n",
    "    \n",
    "    predicitons=[]\n",
    "    print len(predicted_tags)\n",
    "    for row in data_x.itertuples():\n",
    "        if row[2]=='-DOCSTART-':\n",
    "            predicted_tags.insert(row[1],'O')\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    df['id'] = range(len(predicted_tags))\n",
    "    df['tag'] = predicted_tags\n",
    "#     print \"Accuracy score: \" + str(accuracy_score(data_y['tag'],predicted_tags)) \n",
    "    \n",
    "    if mode.lower()=='dev':\n",
    "        df.to_csv('../predictions.csv',index=False)\n",
    "    elif mode.lower()=='test':\n",
    "        df.to_csv('../results/test_y.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = load_data(\"../data/dev_x.csv\", \"../data/dev_y.csv\")\n",
    "test_data = load_data(\"../data/test_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../predictions.csv')\n",
    "df_dev = pd.read_csv('../data/dev_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_dev = pd.read_csv('../data/train_x.csv')\n",
    "df_y_dev = pd.read_csv('../data/dev_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_x_dev.join(df_y_dev, on=\"id\", how = \"inner\", rsuffix = \"_tag\").drop(\"id_tag\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36589147286821705"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_e('There','EX',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>899</td>\n",
       "      <td>closer</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>1216</td>\n",
       "      <td>more</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>2115</td>\n",
       "      <td>earlier</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>2193</td>\n",
       "      <td>earlier</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2267</th>\n",
       "      <td>2267</td>\n",
       "      <td>Earlier</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>2531</td>\n",
       "      <td>More</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>3397</td>\n",
       "      <td>longer</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4829</th>\n",
       "      <td>4829</td>\n",
       "      <td>better</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>5522</td>\n",
       "      <td>more</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6032</th>\n",
       "      <td>6032</td>\n",
       "      <td>earlier</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6876</th>\n",
       "      <td>6876</td>\n",
       "      <td>faster</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>6910</td>\n",
       "      <td>more</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6945</th>\n",
       "      <td>6945</td>\n",
       "      <td>more</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7085</th>\n",
       "      <td>7085</td>\n",
       "      <td>more</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7671</th>\n",
       "      <td>7671</td>\n",
       "      <td>more</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7693</th>\n",
       "      <td>7693</td>\n",
       "      <td>earlier</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>8351</td>\n",
       "      <td>earlier</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9209</th>\n",
       "      <td>9209</td>\n",
       "      <td>more</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9255</th>\n",
       "      <td>9255</td>\n",
       "      <td>more</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9263</th>\n",
       "      <td>9263</td>\n",
       "      <td>further</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10393</th>\n",
       "      <td>10393</td>\n",
       "      <td>more</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10965</th>\n",
       "      <td>10965</td>\n",
       "      <td>more</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11476</th>\n",
       "      <td>11476</td>\n",
       "      <td>more</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11730</th>\n",
       "      <td>11730</td>\n",
       "      <td>more</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12094</th>\n",
       "      <td>12094</td>\n",
       "      <td>more</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13290</th>\n",
       "      <td>13290</td>\n",
       "      <td>closer</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13328</th>\n",
       "      <td>13328</td>\n",
       "      <td>more</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13357</th>\n",
       "      <td>13357</td>\n",
       "      <td>earlier</td>\n",
       "      <td>JJR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13657</th>\n",
       "      <td>13657</td>\n",
       "      <td>earlier</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13839</th>\n",
       "      <td>13839</td>\n",
       "      <td>more</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219854</th>\n",
       "      <td>219854</td>\n",
       "      <td>this</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220200</th>\n",
       "      <td>220200</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220299</th>\n",
       "      <td>220299</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220974</th>\n",
       "      <td>220974</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226503</th>\n",
       "      <td>226503</td>\n",
       "      <td>carping</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226661</th>\n",
       "      <td>226661</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227134</th>\n",
       "      <td>227134</td>\n",
       "      <td>or</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227136</th>\n",
       "      <td>227136</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227943</th>\n",
       "      <td>227943</td>\n",
       "      <td>American</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228352</th>\n",
       "      <td>228352</td>\n",
       "      <td>this</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228484</th>\n",
       "      <td>228484</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228695</th>\n",
       "      <td>228695</td>\n",
       "      <td>expensive</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228938</th>\n",
       "      <td>228938</td>\n",
       "      <td>likely</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230598</th>\n",
       "      <td>230598</td>\n",
       "      <td>recently</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231187</th>\n",
       "      <td>231187</td>\n",
       "      <td>recently</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231419</th>\n",
       "      <td>231419</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231476</th>\n",
       "      <td>231476</td>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231522</th>\n",
       "      <td>231522</td>\n",
       "      <td>this</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231706</th>\n",
       "      <td>231706</td>\n",
       "      <td>can</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231841</th>\n",
       "      <td>231841</td>\n",
       "      <td>this</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233482</th>\n",
       "      <td>233482</td>\n",
       "      <td>than</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234141</th>\n",
       "      <td>234141</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234525</th>\n",
       "      <td>234525</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234970</th>\n",
       "      <td>234970</td>\n",
       "      <td>effective</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237479</th>\n",
       "      <td>237479</td>\n",
       "      <td>than</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237816</th>\n",
       "      <td>237816</td>\n",
       "      <td>than</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237923</th>\n",
       "      <td>237923</td>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238852</th>\n",
       "      <td>238852</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239094</th>\n",
       "      <td>239094</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239600</th>\n",
       "      <td>239600</td>\n",
       "      <td>if</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id       word  tag\n",
       "899        899     closer  JJR\n",
       "1216      1216       more  JJR\n",
       "2115      2115    earlier   RB\n",
       "2193      2193    earlier   RB\n",
       "2267      2267    Earlier   RB\n",
       "2531      2531       More  RBR\n",
       "3397      3397     longer   RB\n",
       "4829      4829     better  JJR\n",
       "5522      5522       more  JJR\n",
       "6032      6032    earlier  RBR\n",
       "6876      6876     faster  RBR\n",
       "6910      6910       more  RBR\n",
       "6945      6945       more  JJR\n",
       "7085      7085       more  RBR\n",
       "7671      7671       more  JJR\n",
       "7693      7693    earlier  JJR\n",
       "8351      8351    earlier  JJR\n",
       "9209      9209       more  JJR\n",
       "9255      9255       more  RBR\n",
       "9263      9263    further   JJ\n",
       "10393    10393       more  RBR\n",
       "10965    10965       more  JJR\n",
       "11476    11476       more  JJR\n",
       "11730    11730       more  JJR\n",
       "12094    12094       more  RBR\n",
       "13290    13290     closer  RBR\n",
       "13328    13328       more  JJR\n",
       "13357    13357    earlier  JJR\n",
       "13657    13657    earlier  RBR\n",
       "13839    13839       more  RBR\n",
       "...        ...        ...  ...\n",
       "219854  219854       this   DT\n",
       "220200  220200          .    .\n",
       "220299  220299          .    .\n",
       "220974  220974          .    .\n",
       "226503  226503    carping   NN\n",
       "226661  226661          ,    ,\n",
       "227134  227134         or   CC\n",
       "227136  227136          ,    ,\n",
       "227943  227943   American   JJ\n",
       "228352  228352       this   DT\n",
       "228484  228484          .    .\n",
       "228695  228695  expensive   JJ\n",
       "228938  228938     likely   JJ\n",
       "230598  230598   recently   RB\n",
       "231187  231187   recently   RB\n",
       "231419  231419          .    .\n",
       "231476  231476         on   IN\n",
       "231522  231522       this   DT\n",
       "231706  231706        can   MD\n",
       "231841  231841       this   DT\n",
       "233482  233482       than   IN\n",
       "234141  234141         in   IN\n",
       "234525  234525          ,    ,\n",
       "234970  234970  effective   JJ\n",
       "237479  237479       than   IN\n",
       "237816  237816       than   IN\n",
       "237923  237923         on   IN\n",
       "238852  238852       said  VBD\n",
       "239094  239094         in   IN\n",
       "239600  239600         if   IN\n",
       "\n",
       "[418 rows x 3 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.loc[df.tag=='RBR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes tested: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-5713a7ef727f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.93\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Indexes tested: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m     71\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/multiclass.pyc\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# Known to fail in numpy 1.3 for array of arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for i in range(len(df)-1):\n",
    "    true = list(df_dev['tag'])\n",
    "    pred = list(df['tag'])\n",
    "    pred = pred[:i]+pred[i+1:]\n",
    "    accuracy = accuracy_score(true,pred)\n",
    "    if accuracy>0.93:\n",
    "        print accuracy,i\n",
    "    accuracies.append(accuracy_score(true,pred))\n",
    "    if i%10000==0:\n",
    "        print \"Indexes tested: \" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
